
# coding: utf-8

# ### AE ON h2t TAKENS DELAY EMBEDDINGS

# ### setup

# In[1]:


import math
from copy import deepcopy
import os
import pickle
import time

import numpy as np

import matplotlib as mpl
if os.environ.get('DISPLAY','') == '':
    print('no display found. Using non-interactive Agg backend')
    mpl.use('Agg')
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from scipy.linalg import orthogonal_procrustes

from sklearn import manifold

import torch
from torch import nn
import torch.nn.functional as F
from torch.autograd import Variable
import torch.utils.data
from torch.utils.data import DataLoader, Dataset
import torch.nn.parallel
import torch.optim as optim
from torch.autograd import Variable

from collections import OrderedDict

import sklearn.preprocessing as pre
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import euclidean_distances

import MDAnalysis as mda
from MDAnalysis.analysis import distances
from MDAnalysis.analysis import align
from MDAnalysis.analysis.rms import rmsd


# In[3]:


showPlots=0
useMagics=0
if useMagics:
    get_ipython().magic(u'matplotlib inline')
    #%matplotlib notebook
    get_ipython().magic(u'load_ext autoreload')
    get_ipython().magic(u"autoreload 2'''")


# In[4]:


if torch.cuda.is_available():
    print("=> Using GPU")
    print("CUDA device count =")
    print torch.cuda.device_count()
    print("Selecting decvice = cuda:0")
    device = torch.device("cuda:0")
    print("Device name = ")
    print torch.cuda.get_device_name(0)
else:
    print("=> Using CPU")
    device = torch.device("cpu")

print("Using device = %s" % device)


# In[5]:


# fix random seed for reproducibility
np.random.seed(200186)
torch.manual_seed(200186)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(200186)


# ### loading h2t_list.pkl holding h2t distances (generated by 0_traj2pdh2t)

# In[6]:


# unpickling h2t_list
file = open('h2t_list.pkl', 'rb')
h2t_list = pickle.load(file)
file.close()


# In[7]:


# printing data
for i in range(0,len(h2t_list)):
    print("frame = %d: h2t = %f A" % (i, h2t_list[i]))


# In[8]:


# no. of frames
nFrames = len(h2t_list)
print('nFrames = %d' % nFrames)


# In[9]:


# converting to numpy array
h2t = np.array(h2t_list)


# ### pre-processing  data

# In[14]:


# specifying delay time tau (in steps) and delay dimensionality dim
tau = 1
dim = 15
print h2t.shape
print h2t


# In[15]:


# constructing delay vectors
delayVecs = np.reshape(h2t[0:len(h2t)-(dim-1)*tau],(len(h2t)-(dim-1)*tau,1))
for ii in range(1,dim):
    delayVecs = np.concatenate((delayVecs,np.reshape(h2t[ii*tau:len(h2t)-(dim-1)*tau+ii*tau],(len(h2t)-(dim-1)*tau,1))), axis=1)
#delayVecs = np.fliplr(delayVecs) # converting to backward delay embeddings [v(t), v(t-tau), v(t-2*tau), ... , v(t-(dim-2)*tau), v(t-(dim-1)*tau)]
print delayVecs
print delayVecs.shape


# In[16]:


# subsampling delay vectors
frameIdx = np.arange(delayVecs.shape[0])
starter=0
stopper=50000
skipper=2#1
frameIdx = frameIdx[starter:stopper:skipper]
x = deepcopy(delayVecs[starter:stopper:skipper])
y = deepcopy(x)


# In[17]:


# - data augmentation 
# -- assuming data is from equilibrium ensemble and detailed balance is satisfied
# -- impose temporal invariance on learning such that fwd & bkwd delay vectors are equivalent by data augmentation
x_aug = np.fliplr(x)
x = np.vstack((x,x_aug))

y = np.vstack((y,y))

print x.shape
print y.shape


# In[18]:


# train, val, test split
test_frac = 0.10
val_frac = 0.20
frameIdx = np.hstack((frameIdx,frameIdx))
xInverted = np.hstack((np.zeros(x.shape[0]/2),np.ones(x.shape[0]/2))).astype(int) # flag indicating whether regular or inverted delay embedding training point
x_train_val, x_test, y_train_val, y_test, frameIdx_train_val, frameIdx_test, xInverted_train_val, xInverted_test = train_test_split(x, y, frameIdx, xInverted, test_size=test_frac, random_state=42)
#x_train, x_val, y_train, y_val, frameIdx_train, frameIdx_val, xInverted_train, xInverted_val = train_test_split(x_train_val, y_train_val, frameIdx_train_val, xInverted_train_val, test_size=val_frac/(1-test_frac), random_state=43)


# In[19]:


# train+val, test, all scaling
xscale = pre.MinMaxScaler(feature_range=(0,1)) # changing feature range requires modification of chain rule in post-hoc gradient rescaling
yscale = pre.MinMaxScaler(feature_range=(0,1)) # changing feature range requires modification of chain rule in post-hoc gradient rescaling

X_train_val = xscale.fit_transform(x_train_val)
Y_train_val = yscale.fit_transform(y_train_val)
X_test = xscale.transform(x_test)
Y_test = yscale.transform(y_test)
X = xscale.transform(x)
Y = yscale.transform(y)


# ### initializing AE

# In[20]:


# AE: h2t_delay -> latent space -> h2t_delay_r

class AE_net(nn.Module):
    def __init__(self, q, d):
        super(AE_net, self).__init__()
        self.hiddenEncode1 = nn.Linear(dim, np.int(np.ceil(q*dim)))
        self.hiddenEncode2 = nn.Linear(np.int(np.ceil(q*dim)), np.int(np.ceil(q*dim)))
        self.bottleneck = nn.Linear(np.int(np.ceil(q*dim)),d)
        self.hiddenDecode1 = nn.Linear(d, np.int(np.ceil(q*dim)))
        self.hiddenDecode2 = nn.Linear(np.int(np.ceil(q*dim)), np.int(np.ceil(q*dim)))
        self.output = nn.Linear(np.int(np.ceil(q*dim)), dim)
        
    def forward(self, x):
        x = torch.tanh(self.hiddenEncode1(x))
        x = torch.tanh(self.hiddenEncode2(x))
        x = torch.tanh(self.bottleneck(x))
        x = torch.tanh(self.hiddenDecode1(x))
        x = torch.tanh(self.hiddenDecode2(x))
        x = torch.tanh(self.output(x))
        return x

# https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5
activation = {}
def get_activation(name):
    def hook(model, input, output):
        activation[name] = output.cpu().detach()
    return hook

def get_var_from_np(np_array, requires_grad=False):    # convert numpy to PyTorch variable for training
    temp = Variable(torch.from_numpy(np_array), requires_grad=requires_grad).type(torch.FloatTensor)
    if torch.cuda.is_available(): 
        temp = temp.cuda()
    temp = temp.to(device)
    return temp
    
def train(model, n_epoch, batch_size, data_in, data_out, data_in_val, data_out_val):
    
    train_data = My_dataset(get_var_from_np(data_in), get_var_from_np(data_out))
    val_data = My_dataset(get_var_from_np(data_in_val), get_var_from_np(data_out_val))
    
    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, weight_decay=0)
    
    n_batch = math.ceil(1. * len(train_data) / batch_size)
    
    loss_train = np.zeros(n_epoch)
    loss_val = np.zeros(n_epoch)
    
    for epoch in range(n_epoch):    
        
        dataset = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)
        
        batch_idx = 0
        for train_in, train_out in dataset:
            out = model(train_in)
            loss = nn.MSELoss()(out, train_out) 
            #if (batch_idx+1) % min(10,n_batch) == 0:
            #    print "Epoch = %5d, Batch = %5d/%5d [%4.0f%%]:\tTraining Loss = %12.5e" % (epoch, batch_idx+1, n_batch, 100. * (batch_idx+1) / n_batch, loss.data.numpy())
            optimizer.zero_grad()
            loss.backward() # scalar variables, when we call .backward() on them, don’t require arguments – only tensors require a matching sized tensor argument to be passed to the .backward() operation
            optimizer.step()
            batch_idx += 1
        
        data_out_pred = model(get_var_from_np(data_in)).cpu().detach().numpy()
        diff = data_out_pred - data_out
        MSE = np.mean(diff**2)
        loss_train[epoch] = MSE
        
        data_out_val_pred = model(get_var_from_np(data_in_val)).cpu().detach().numpy()
        diff = data_out_val_pred - data_out_val
        MSE = np.mean(diff**2)
        loss_val[epoch] = MSE        
        
        if (epoch % 50 == 0):
            print('====> Epoch %d of %d done! Train loss = %.2e, Val loss = %.2e' % (epoch,n_epoch,loss_train[epoch],loss_val[epoch]))
    
    return loss_train, loss_val
    
class My_dataset(Dataset):      # construct dataset object for mini-batch gradient descent
    def __init__(self, data_in, data_out):
        self._data_in = data_in
        self._data_out = data_out

    def __len__(self):
        return len(self._data_in)

    def __getitem__(self, index):
        return self._data_in[index], self._data_out[index]


# In[21]:


def plot_training(loss_train, figname):
    
    '''
    # plotting training and validation losses over epochs of training course
    loss_train - list of training loss at each epoch of training
    loss_val - list of validation loss at each epoch of training
    ''' 
    fig, ax = plt.subplots()
    ax.plot(np.arange(len(loss_train)), loss_train, 'b--')
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_yscale("log")
    ax.legend(["train"])
    if showPlots:
        plt.draw()
        plt.show()
    fig.savefig(figname, dpi=300)
    plt.close()


# In[22]:


def plot_training_validation(loss_train, loss_val, figname):
    
    '''
    # plotting training and validation losses over epochs of training course
    loss_train - list of training loss at each epoch of training
    loss_val - list of validation loss at each epoch of training
    ''' 
    fig, ax = plt.subplots()
    ax.plot(np.arange(len(loss_train)), loss_train, 'b--')
    ax.plot(np.arange(len(loss_val)), loss_val, 'r--')
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_yscale("log")
    ax.legend(["train","val"])
    if showPlots:
        plt.draw()
        plt.show()
    fig.savefig(figname, dpi=300)
    plt.close()


# ### training final model at optimal d and n_epoch

# In[26]:


# training final VAE at optimal d to determine optimal n_epoch

start = time.time()

n_epoch = 500
batch_size = 100
q=2.5
d=2

# train, val split not required as training over train+val partition, perform anyway to provide X_val, Y_val to training subroutine
x_train, x_val, y_train, y_val, frameIdx_train, frameIdx_val, xInverted_train, xInverted_val = train_test_split(x_train_val, y_train_val, frameIdx_train_val, xInverted_train_val, test_size=val_frac/(1-test_frac), random_state=np.random.randint(10000))

X_train = xscale.fit_transform(x_train)
Y_train = yscale.fit_transform(y_train)
X_val = xscale.transform(x_val)
Y_val = yscale.transform(y_val)

model = AE_net(q,d).to(device)
print(model)

loss_train, loss_val = train(model, n_epoch, batch_size, X_train, Y_train, X_val, Y_val)

plot_training_validation(loss_train,loss_val,'dOpt_trainingValidationLoss_Takens.png')

end = time.time()
print("Using device = %s" % device)
print("Elapsed time %.2f (s)" % (end - start))


# In[27]:


# training final VAE over all train_val data at optimal d and n_epoch

start = time.time()

n_epoch = 200
batch_size = 100
q=2.5
d=2

# train, val split not required as training over train+val partition, perform anyway to provide X_val, Y_val to training subroutine
x_train, x_val, y_train, y_val, frameIdx_train, frameIdx_val, xInverted_train, xInverted_val = train_test_split(x_train_val, y_train_val, frameIdx_train_val, xInverted_train_val, test_size=val_frac/(1-test_frac), random_state=np.random.randint(10000))

X_train = xscale.fit_transform(x_train)
Y_train = yscale.fit_transform(y_train)
X_val = xscale.transform(x_val)
Y_val = yscale.transform(y_val)

model = AE_net(q,d).to(device)
print(model)

loss_train, _ = train(model, n_epoch, batch_size, X_train_val, Y_train_val, X_val, Y_val)

plot_training(loss_train,'dOpt_trainingLoss_Takens.png')

end = time.time()
print("Using device = %s" % device)
print("Elapsed time %.2f (s)" % (end - start))


# ### saving model

# In[28]:


# save/load only the model parameters (preferred solution)
save_path = "./model_Takens.pyt"
torch.save(model.state_dict(), save_path)


# ### loading model

# In[29]:


save_path = "./model_Takens.pyt"
if torch.cuda.is_available():
    model.load_state_dict(torch.load(save_path))
else:
    model.load_state_dict(torch.load(save_path, map_location='cpu'))
model.eval()


# ### testing model

# In[30]:


# test set predictions
Y_test_pred = model(get_var_from_np(X_test.astype(np.float32), requires_grad=True)).cpu().detach().numpy()


# In[31]:


# scaled MSE -- AE perspective
diff = Y_test_pred - Y_test
MSE = np.mean(diff**2)
print "MSE_test = %f" % (MSE)
print "RMSE_test = %f" % (np.sqrt(MSE))


# In[34]:


# unscaled RMSE in atomic space
diff = yscale.inverse_transform(Y_test_pred) - yscale.inverse_transform(Y_test)
MSE = np.mean(diff**2)
print "MSE_test = %f" % (MSE) + ' ' + u'\u212B\u00B2'.encode('utf-8')
print "RMSE_test = %f" % (np.sqrt(MSE)) + ' ' + u'\u212B'.encode('utf-8')

